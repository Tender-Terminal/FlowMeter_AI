{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext nb_black\n\n# Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    roc_auc_score,\n    recall_score,\n    f1_score,\n)\nimport warnings\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Function to load and clean data\ndef load_and_clean_data(folder, fname_benign, fname_malicious):\n    benign_data = pd.read_csv(folder + fname_benign)\n    malicious_data = pd.read_csv(folder + fname_malicious)\n\n    # Drop rows with missing values\n    benign_data.dropna(inplace=True)\n    malicious_data.dropna(inplace=True)\n\n    benign_data[\"Type\"] = \"Benign\"\n    malicious_data[\"Type\"] = \"Malicious\"\n\n    combined_data = pd.concat([malicious_data, benign_data])\n    combined_data = combined_data.sample(frac=1)  # Random shuffle\n\n    return combined_data\n\n# Function to add throughput columns\ndef add_throughput_columns(df):\n    colsPerTime = [\n        \"flowLength\",\n        \"fwdFlowLength\",\n        \"bwdFlowLength\",\n        \"packetSizeTotal\",\n        \"fwdPacketSizeTotal\",\n        \"bwdPacketSizeTotal\",\n    ]\n\n    for feature in colsPerTime:\n        df[feature + \"PerTime\"] = df[feature] / df[\"flowDuration\"]\n        print(feature + \"PerTime\")\n\n# Function to clean the dataset\ndef clean_dataset(df):\n    df.dropna(inplace=True)\n    df_X = df.iloc[:, :-1]\n    df_Y = df.iloc[:, -1]\n\n    indices_to_keep = ~df_X.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df_X[indices_to_keep].astype(np.float64).values, df_Y[indices_to_keep].values\n\n# Load and clean the data\nfolder = \"../pkg/flowOutput/\"\nfname_benign = \"2017-05-02_kali-normal22_flow_stats.csv\"\nfname_malicious = \"webgoat_flow_stats.csv\"\n\ncombined_data = load_and_clean_data(folder, fname_benign, fname_malicious)\n\n# Add throughput columns\nadd_throughput_columns(combined_data)\n\n# Define feature columns\nfeature_cols = [\n    # Your feature columns here\n]\n\n# Select feature columns in datasets\npd_comb_features = combined_data[feature_cols]\n\n# Get feature and class arrays\nX, y = clean_dataset(pd_comb_features.copy(deep=True))\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Scale the data\nscaler = StandardScaler()\nX_train_scale = scaler.fit_transform(X_train)\nX_test_scale = scaler.transform(X_test)\n\n# Weighted Logistic Regression\n# Define hyperparameter grid\nhyperparam_grid = {\n    # Your hyperparameters here\n}\n\n# Model fitting\nlg = LogisticRegression(random_state=13)\ngrid = GridSearchCV(lg, hyperparam_grid, scoring=\"roc_auc\", cv=10, n_jobs=-1, refit=True)\ngrid.fit(X_train_scale, y_train2.astype(\"int32\"))\n\n# Print best score and parameters\nprint(f\"Best score: {grid.best_score_} with param: {grid.best_params_}\")\n\n# Test performance\ny_pred_wt = grid.predict(X_test_scale)\n\n# Performance metrics\nconf_mat = confusion_matrix(y_test2.astype(\"int32\"), y_pred_wt)\nprint(f\"Accuracy Score: {accuracy_score(y_test2.astype('int32'), y_pred_wt)}\")\nprint(f\"Confusion Matrix:\\n{confusion_matrix(y_test2.astype('int32'), y_pred_wt)}\")\nprint(f\"Area Under Curve: {roc_auc_score(y_test2.astype('int32'), y_pred_wt)}\")\nprint(f\"Recall score (Pct of true malicious detected): {100 * recall_score(y_test2.astype('int32'), y_pred_wt)}\")\nprint(f\"Data reduction: {np.round(100.0 * conf_mat.T[1].sum() / conf_mat.sum(), 2)} percent\")\nprint(f\"Pct malicious in data sent to console: {np.round(100.0 * conf_mat.T[1][1] / conf_mat.T[1].sum(), 2)} percent\")\nprint(\"F1 score:\", f1_score(y_test2.astype(\"int32\"), y_pred_wt, average=\"weighted\"))\n\n# Save parameters\nnp.savetxt(\"mean.txt\", scaler.mean_, delimiter=\",\")\nnp.savetxt(\"std.txt\", scaler.scale_, delimiter=\",\")\nnp.savetxt(\"weights.txt\", best_fit_model.coef_[0], delimiter=\",\")\nnp.savetxt(\"intercept.txt\", best_fit_model.intercept_, delimiter=\",\")\n\n# Feature importance scores\nimportant_features = pd_comb_features_cp.iloc[:, :-1].columns.values[np.argsort(-1 * np.abs(best_fit_model.coef_[0])]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}